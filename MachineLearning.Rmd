---
title: "Machine Learning Final Project, Coursera"
author: "Samer Alwan"
date: "August 25, 2018"
output: html_document
---
#Synopsis
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 
In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. We will then predict this classe variable from the test set.

#Study Design
The test dataset provided for this project does not contain the classe variable anymore so we could not infer the accuracy of the model from it. We therefore split the training dataset into three subsets: training, test and validation. The training set was used to build the model, the test set was meant for eventual tuning which did not turn out to be necessary and the validation set was used for the final testing of the model and assessing its accuracy.
The random forest method was used. The particular reason was that this model is suitable for predicting categorical variables with continuous ones. The model is also very powerfull and relatively simple.

Note on cross-validation: the k-fold method had been tried with three folds, but the result was three models with nearly identical accuracy close to 100%. It did not seem to have any added value and it was not used in the project.


#Data cleaning
Loading the data and checking its structure:
```{r}
setwd("~/Documents/Coursera/MachineLearningProject")
data <- read.csv("pml-training.csv")
datatest <- read.csv("pml-testing.csv")
str(data)
str(datatest)
```
Looking at the missing values:
```{r}
a <- sapply(data, function(x) sum(is.na(x)))
b <- sapply(datatest, function(x) sum(is.na(x)))
a;b
```
As we can see, the count of NAs in any column where they appear is equal or nearly equal to the number of rows in the entire dataset. It means that there is no point in imputing missing values or deleting just individual observations. We can simply delete all the columns with NAs. These columns are not identical for training set and test set and we therefore need to remove all the columns from both sets so they have the same structure.
```{r}
x <- names(a[a>0])
y <- names(b[b>0])
data <- data[, !names(data) %in% x]
data <- data[, !names(data) %in% y]
datatest <- datatest[, !names(datatest) %in% x]
datatest <- datatest[, !names(datatest) %in% y]
```
Finally, we will remove the first seven columns from both sets because, according to their names, they do not have any informational value.
```{r}
data <- data[, -(1:7)]
datatest <- datatest[, -(1:7)]
```

#Model Building and Evaluation
Splitting the training dataset as per the Study Design above:
```{r, message = FALSE}
library(caret)
set.seed(1245)
split1 <- createDataPartition(y = data$classe, p = 0.7, list = FALSE)
data1 <- data[split1,]
validation <- data[-split1,]

split2 <- createDataPartition(y=data1$classe, p=0.7, list = FALSE)
train <- data1[split2,]
test <- data1[-split2,]
```
The ranger package was used to build the model. It is an exclusively random-forest package and it performs much faster than caret.
```{r}
library(ranger)
model <- ranger(classe~., data = train)
```
In-sample accuracy:
```{r}
predtrain <- predict(model, data = train)
confusionMatrix(data = predtrain$predictions, reference = train$classe)
```
Out-of-sample accuracy:
```{r}
predtest <- predict(model, data = test)
confusionMatrix(data = predtest$predictions, reference = test$classe)
```
As we can see, the in-sample accuracy is 100% and the ou-of-sample accuracy is 99.2%, therefore the in-sample error is 0 and the our-of-sample error is 0.8%. This is an acceptable error increase and does not signalize model overfitting.
Therefore, the model does not require any tuning and we can perform the final validation.

#Model Validation
```{r}
predvalid <- predict(model, data = validation)
confusionMatrix(data = predvalid$predictions, reference = validation$classe)
```
The final model accuracy is estimated to 99.29%.

#Final Prediction
```{r}
finalprediction <- predict(model, data = datatest)
finalprediction$predictions
```
